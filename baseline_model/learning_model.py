import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple


class SimpleMLP(nn.Module):
    """
    æ™®é€š MLPï¼Œè‡ªåŠ¨å¯¹å…¨0è¾“å…¥åŠ  maskï¼Œå¹¶å¸¦ LayerNorm
    """
    def __init__(self, hidden_dim, output_dim, eps=1e-6, dropout=0.1):
        super().__init__()
        self.eps = eps
        self.model = nn.Sequential(
            nn.LazyLinear(hidden_dim),
            nn.LayerNorm(hidden_dim),   # åŠ å…¥ LayerNorm
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x_list):
        """
        x_list: [tensor1, tensor2, ...] å„ [B, d_i]
        è¿”å›: out:[B,output_dim], mask:[B,1]
        mask = 1 è¡¨ç¤ºæœ‰æ•ˆè¾“å…¥ï¼Œmask = 0 è¡¨ç¤ºå…¨0
        """
        x = torch.cat(x_list, dim=-1)   # æ‹¼æ¥è¾“å…¥
        mask = (x.abs().sum(dim=-1, keepdim=True) > self.eps).float()
        safe_x = x * mask               # å¯¹å…¨0è¾“å…¥ç½® 0
        out = self.model(safe_x)
        return out, mask
    

class CrossABThenMLP(nn.Module):
    """
    è¾“å…¥: [t, A, B]
      - t: [B, h]
      - A: [B, T_a, h]
      - B: [B, T_b, h]
    æµç¨‹:
      1) Aâ†B, Bâ†A äº’ç›¸ cross-attention
      2) å¯¹ A' ä¸ B' åšå¹³å‡æ± åŒ– (å¯è‡ªåŠ¨maskå…¨é›¶token)
      3) æ‹¼æ¥ [t, a_pool, b_pool] -> LayerNorm -> MLP -> é¢„æµ‹
    """
    def __init__(self,
                 h: int,
                 nhead: int = 4,
                 hidden: int = 512,
                 out_dim: int = 1,
                 dropout: float = 0.1,
                 use_auto_mask: bool = True,
                 zero_eps: float = 1e-8):
        super().__init__()
        self.use_auto_mask = use_auto_mask
        self.zero_eps = zero_eps

        self.attn_ab = nn.MultiheadAttention(embed_dim=h * 2, num_heads=nhead,
                                             dropout=dropout, batch_first=True)
        self.attn_ba = nn.MultiheadAttention(embed_dim=h * 2, num_heads=nhead,
                                             dropout=dropout, batch_first=True)

        # æ‹¼æ¥åç»´åº¦ä¸º 3hï¼Œå…ˆåš LayerNorm å†è¿‡ MLP
        self.mlp = nn.Sequential(
            nn.LazyLinear(hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, out_dim)
        )

    @staticmethod
    def _auto_padding_mask(x: torch.Tensor, eps: float) -> torch.Tensor:
        """
        x: [B, T, h] -> è¿”å› key_padding_mask [B, T] (True=maskè¯¥token)
        ä¾æ®: token å…¨é›¶/è¿‘ä¼¼å…¨é›¶åˆ™è§†ä½œ padding
        """
        return (x.abs().sum(dim=-1) <= eps)

    def forward(self, inputs: List[torch.Tensor]):
        assert len(inputs) == 3, "inputs åº”ä¸º [t, A, B]"
        t, A, B = inputs

        # --- 1) è‡ªåŠ¨maskï¼ˆå¯é€‰ï¼‰ ---
        kpm_A = self._auto_padding_mask(A, self.zero_eps) if self.use_auto_mask else None  # [B, T_a]
        kpm_B = self._auto_padding_mask(B, self.zero_eps) if self.use_auto_mask else None  # [B, T_b]

        # --- 2) äº’ç›¸ cross-attention ---
        # A' = Attn(query=A, key=B, value=B)
        A_out, _ = self.attn_ab(query=A, key=B, value=B, key_padding_mask=kpm_B)
        # B' = Attn(query=B, key=A, value=A)
        B_out, _ = self.attn_ba(query=B, key=A, value=A, key_padding_mask=kpm_A)

        # --- 3) æ‹¼æ¥ -> LayerNorm -> MLP ---
        fused = torch.cat([t, A_out, B_out], dim=-1)          
        fused_dim = fused.size(-1)
        self.pre_ln = nn.LayerNorm(fused_dim).to(device=fused.device, dtype=fused.dtype)        
        pred = self.mlp(fused)                                   # [B, out_dim]

        return pred, fused
    

class LabelBiasedSelfAttnMeanPooling(nn.Module):
    """
    Label-Biased Self-Attention + Mean Pooling
    - 3D è¾“å…¥: [B, T, D]   (pool_dim=1)
    - 4D è¾“å…¥: [B, G, T, D](pool_dim=2ï¼Œå¯¹æ¯ä¸ª G ç‹¬ç«‹æ²¿ T åš self-attn + pooling)
    - é€šè¿‡ label ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ï¼Œåç½® query:  q_bias = f(label)  â†’  mha(query = x + q_bias, key=x, value=x)
    - å¿½ç•¥å…¨0 tokenï¼ˆkpmï¼‰å¹¶åœ¨ pooling æ—¶ä»…å¯¹æœ‰æ•ˆ token æ±‚å¹³å‡
    """
    def __init__(self,
                 embed_dim: int,
                 num_heads: int = 1,
                 attn_dropout: float = 0.0,
                 label_hidden: int = 64,
                 bias_scale_init: float = 1.0):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        # è‡ªæ³¨æ„åŠ›
        self.mha = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=attn_dropout,
            batch_first=True,  # [N, T, D]
        )

        # ç”¨ label ç”Ÿæˆ query åç½®å‘é‡ q_bias: [NG, D]
        # è¿™é‡ŒæŠŠæ ‡é‡/å°å‘é‡çš„ label æŠ•å½±ä¸ºä¸ embed_dim å¯¹é½çš„å‘é‡
        self.label_to_qbias = nn.Sequential(
            nn.LayerNorm(1),                  # label å½’ä¸€åŒ–ï¼ˆæ ‡é‡ä¹Ÿå¯ç¨³å®šä¸€äº›ï¼‰
            nn.Linear(1, label_hidden),
            nn.ReLU(),
            nn.Linear(label_hidden, embed_dim)
        )
        # å¯å­¦ä¹ ç¼©æ”¾ï¼Œæ§åˆ¶ label åç½®å¼ºåº¦
        self.bias_scale = nn.Parameter(torch.tensor(bias_scale_init, dtype=torch.float32))

    @staticmethod
    def _kpm_from_embs(x: torch.Tensor, eps: float):
        # x: [..., T, D] -> [ ..., T ]  True=éœ€è¦å±è”½ï¼ˆå…¨0ï¼‰
        return (x.abs().sum(dim=-1) <= eps)

    def _prep_labels(self, labels: torch.Tensor, B: int, G: int, device, dtype,
                    reduce: str = "mean"):
        """
        ç›®æ ‡ï¼šè¿”å› [B, G, 1]
        å…è®¸è¾“å…¥ï¼š
        [B], [B,1], [B,G], [B,G,1], [B,1,1],
        [B,T], [B,G,T],
        [B,G,T,1]  <-- æ–°å¢æ”¯æŒ
        reduce: å½“å¸¦ T ç»´æ—¶çš„é™ç»´æ–¹å¼ï¼š'mean' | 'last' | 'sum'
        """
        L = labels.to(device=device, dtype=dtype)

        # å…ˆæŠŠæœ«å°¾å¤šä½™çš„ size=1 ç»´å»æ‰ä¸€å±‚ï¼ˆé¿å… [B,G,T,1] é˜»ç¢åˆ¤æ–­ï¼‰
        if L.dim() >= 2 and L.size(-1) == 1:
            L = L[..., 0]  # e.g. [B,G,T,1]->[B,G,T], [B,G,1]->[B,G], [B,1]->[B]

        if L.dim() == 1:
            # [B] -> [B,G,1]
            assert L.size(0) == B, f"labels batch={L.size(0)} ä¸ B={B} ä¸ç¬¦"
            L = L.view(B, 1, 1).expand(B, G, 1)

        elif L.dim() == 2:
            # [B,1] æˆ– [B,G] æˆ– [B,T]
            assert L.size(0) == B, f"labels batch={L.size(0)} ä¸ B={B} ä¸ç¬¦"
            if L.size(1) == 1:
                # [B,1] -> [B,G,1]
                L = L.view(B, 1, 1).expand(B, G, 1)
            elif L.size(1) == G:
                # [B,G] -> [B,G,1]
                L = L.unsqueeze(-1)
            else:
                # è§†ä¸º [B,T]ï¼šæ²¿ T é™ç»´ -> [B,1] -> [B,G,1]
                if reduce == "mean":
                    L = L.mean(dim=1, keepdim=True)
                elif reduce == "last":
                    L = L[:, -1:].contiguous()
                elif reduce == "sum":
                    L = L.sum(dim=1, keepdim=True)
                else:
                    raise ValueError(f"æœªçŸ¥ reduce: {reduce}")
                L = L.view(B, 1, 1).expand(B, G, 1)

        elif L.dim() == 3:
            # [B,G] / [B,G,T] / [B,1,1]
            assert L.size(0) == B, f"labels batch={L.size(0)} ä¸ B={B} ä¸ç¬¦"
            if L.size(1) == G and L.size(2) == 1:
                # [B,G,1] -> OK
                pass
            elif L.size(1) == 1 and L.size(2) == 1:
                # [B,1,1] -> [B,G,1]
                L = L.expand(B, G, 1)
            elif L.size(1) == G:
                # [B,G,T]ï¼šæ²¿ T é™ç»´ -> [B,G,1]
                if reduce == "mean":
                    L = L.mean(dim=2, keepdim=True)
                elif reduce == "last":
                    L = L[:, :, -1:].contiguous()
                elif reduce == "sum":
                    L = L.sum(dim=2, keepdim=True)
                else:
                    raise ValueError(f"æœªçŸ¥ reduce: {reduce}")
            else:
                raise ValueError(f"æ— æ³•è§£é‡Šçš„ labels å½¢çŠ¶: {tuple(L.shape)} (æœŸæœ›ç¬¬äºŒç»´=G)")

        elif L.dim() == 4:
            # ä½ å½“å‰æŠ¥é”™çš„æƒ…å†µï¼š[B,G,T,1] -> squeeze last -> [B,G,T]ï¼Œå†æŒ‰ 3D é€»è¾‘å¤„ç†
            assert L.size(0) == B and L.size(1) == G, \
                f"labels å½¢çŠ¶ {tuple(L.shape)} ä¸ B={B}, G={G} ä¸ç¬¦"
            # å·²ç»åœ¨æœ€å¼€å§‹ squeeze æ‰äº†æœ€åä¸€ç»´ï¼Œè¿™é‡Œåº”è¯¥å˜æˆ [B,G,T]
            if reduce == "mean":
                L = L.mean(dim=2, keepdim=True)   # [B,G,1]
            elif reduce == "last":
                L = L[:, :, -1:].contiguous()     # [B,G,1]
            elif reduce == "sum":
                L = L.sum(dim=2, keepdim=True)    # [B,G,1]
            else:
                raise ValueError(f"æœªçŸ¥ reduce: {reduce}")

        else:
            raise ValueError(f"labels ç»´åº¦ {L.dim()} ä¸æ”¯æŒ: å½¢çŠ¶ {tuple(L.shape)}")

        return L  # [B,G,1]

    def forward(self,
                embs: torch.Tensor,
                labels: torch.Tensor,
                pool_dim: int = 1,
                eps: float = 1e-6,
                keepdim: bool = False) -> torch.Tensor:
        """
        embs: 3D [B,T,D] æˆ– 4D [B,G,T,D]
        labels: ä¸ batch å¯¹é½çš„æ”¶ç›Šç‡æ ‡ç­¾ï¼ˆè§ _prep_labels æ”¯æŒçš„å½¢çŠ¶ï¼‰
        è¿”å›ï¼š
          3D: [B, D] æˆ– [B, 1, D] (keepdim=True)
          4D: [B, G, D] æˆ– [B, G, 1, D] (keepdim=True)
        """
        if embs.dim() == 3:
            B, T, D = embs.shape
            G = 1
            x4 = embs.unsqueeze(1)               # [B, 1, T, D]
            squeeze_3d = True
            assert pool_dim == 1, "pool_dim = 1 for 3D input"
        elif embs.dim() == 4:
            B, G, T, D = embs.shape
            x4 = embs                              # [B, G, T, D]
            squeeze_3d = False
            assert pool_dim == 2, "pool_dim = 2 for 4D input"
        else:
            raise ValueError("åªæ”¯æŒ 3D [B,T,D] æˆ– 4D [B,G,T,D]")

        device, dtype = embs.device, embs.dtype

        # 1) key padding mask: True è¡¨ç¤ºè¯¥ token è¢«å¿½ç•¥ï¼ˆå…¨é›¶ï¼‰
        kpm4 = self._kpm_from_embs(x4, eps=eps)        # [B, G, T]

        # 2) å±•å¹³ B ä¸ G æ–¹ä¾¿é€å…¥ MHA
        NG = B * G
        x = x4.reshape(NG, T, D).contiguous()          # [NG, T, D]
        kpm = kpm4.reshape(NG, T).contiguous()         # [NG, T]

        # 3) å¤„ç† labels -> ç”Ÿæˆ query åç½®å‘é‡ q_bias: [NG, D]
        L = self._prep_labels(labels, B, G, device, dtype)   # [B,G,1]
        L_flat = L.reshape(NG, 1)                            # [NG, 1]
        q_bias = self.label_to_qbias(L_flat)                 # [NG, D]
        q_bias = (self.bias_scale * q_bias).unsqueeze(1)     # [NG, 1, D] å¯¹æ•´æ¡åºåˆ—å…±äº«

        # 4) Label-biased self-attention
        #    åœ¨ query ä¾§åŠ åç½®ï¼šquery = x + q_bias
        attn_out, _ = self.mha(query=x + q_bias, key=x, value=x, key_padding_mask=kpm)  # [NG, T, D]

        # 5) æ•°å€¼æ¸…æ´—
        attn_out = torch.nan_to_num(attn_out, nan=0.0, posinf=0.0, neginf=0.0)

        # 6) token çº§æœ‰æ•ˆæ€§ï¼šè¯¥ token é padding ä¸”è¾“å‡º finite
        token_all_finite = torch.isfinite(attn_out).all(dim=-1)        # [NG, T]
        token_valid = (~kpm) & token_all_finite                        # [NG, T]
        mask = token_valid.unsqueeze(-1).float()                       # [NG, T, 1]

        # 7) ä»…å¯¹æœ‰æ•ˆ token åš mean poolingï¼›è‹¥æ— æœ‰æ•ˆ tokenï¼Œåˆ™è¾“å‡º 0
        safe = attn_out * mask                                         # [NG, T, D]
        num = safe.sum(dim=1, keepdim=keepdim)                         # [NG, D] æˆ– [NG, 1, D]
        den = mask.sum(dim=1, keepdim=keepdim).clamp(min=1.0)
        pooled = num / den                                             # [NG, D] æˆ– [NG, 1, D]

        # 8) è¿˜åŸå½¢çŠ¶
        if keepdim:
            pooled = pooled.reshape(B, G, 1, D)                        # [B, G, 1, D]
            if squeeze_3d:
                pooled = pooled[:, 0:1, :, :].squeeze(1)               # [B, 1, D]
        else:
            pooled = pooled.reshape(B, G, D)                           # [B, G, D]
            if squeeze_3d:
                pooled = pooled[:, 0, :]                               # [B, D]

        return pooled


class LabelTempBiasSelfAttn(nn.Module):
    """
    Label-biased Self-Attention
    - ä¸¤ä¸ªå¯å­¦ä¹ æ˜ å°„:
        1) tau(label): æ ‡é‡æ¸©åº¦ï¼Œæ§åˆ¶åˆ†å¸ƒå°–/å¹³
        2) b_key(label): åˆ—åç½®å‘é‡ï¼ˆé•¿åº¦ Tï¼‰ï¼Œå¯¹æ¯ä¸€è¡Œ logits åŠ åŒä¸€åˆ—å‘é‡
    - è¾“å…¥:
        x: [B,T,D] æˆ– [B,G,T,D]
        labels: [B]/[B,1]/[B,G]/[B,G,1]/[B,T]/[B,G,T]/[B,G,T,1]
        key_padding_mask(å¯é€‰): [B,T] æˆ– [B,G,T]ï¼ŒTrue=å±è”½
    - è¾“å‡º:
        è‹¥ return_seq=True: ä¸ x åŒå½¢çŠ¶ï¼ˆ[B,T,D] æˆ– [B,G,T,D]ï¼‰
        è‹¥ return_seq=False: mean pooling åçš„ [B,D] æˆ– [B,G,D]
        å¯é€‰è¿”å› attn_weightsï¼ˆå¹³å‡åˆ° head åçš„ [B,T,T] æˆ– [B,G,T,T]ï¼‰
    """
    def __init__(self, d_model: int, nhead: int = 4,
                 temp_hidden: int = 32,
                 bias_hidden: int = 64,
                 dropout: float = 0.1,
                 max_len: int = 1024,
                 reduce: str = 'mean',
                 return_seq: bool = False,
                 return_attn: bool = False):
        super().__init__()
        assert d_model % nhead == 0, "d_model å¿…é¡»èƒ½è¢« nhead æ•´é™¤"
        assert reduce in ('mean', 'last')
        self.d_model = d_model
        self.nhead = nhead
        self.head_dim = d_model // nhead
        self.do = nn.Dropout(dropout)
        self.reduce = reduce
        self.return_seq = return_seq
        self.return_attn = return_attn
        self.tau_eps = 1e-3

        # QKV
        self.Wq = nn.Linear(d_model, d_model, bias=False)
        self.Wk = nn.Linear(d_model, d_model, bias=False)
        self.Wv = nn.Linear(d_model, d_model, bias=False)
        self.Wo = nn.Linear(d_model, d_model, bias=False)

        # tau(label) æ ‡é‡
        self.tau_mlp = nn.Sequential(
            nn.LayerNorm(1),
            nn.Linear(1, temp_hidden), nn.ReLU(),
            nn.Linear(temp_hidden, 1)
        )

        # b_key(label) æ ‡é‡ â†’ ä¹˜ä»¥å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥å¾—åˆ°åˆ—åç½®
        self.bias_mlp = nn.Sequential(
            nn.LayerNorm(1),
            nn.Linear(1, bias_hidden), nn.ReLU(),
            nn.Linear(bias_hidden, 1)
        )
        # ä½ç½®åµŒå…¥ï¼ˆä¼šæŒ‰éœ€è‡ªåŠ¨æ‰©å®¹ï¼‰
        self.register_parameter('pos_embed', nn.Parameter(torch.zeros(1, max_len)))
        nn.init.normal_(self.pos_embed, std=0.02)

    # ---------- utils ----------
    @staticmethod
    def _merge_BG(x: torch.Tensor) -> Tuple[torch.Tensor,int,int,int,int,bool]:
        if x.dim() == 3:
            B, T, D = x.shape
            G = 1
            return x, B, G, T, D, True
        elif x.dim() == 4:
            B, G, T, D = x.shape
            X = x.reshape(B*G, T, D).contiguous()
            return X, B, G, T, D, False
        else:
            raise ValueError("x must be [B,T,D] or [B,G,T,D]")

    @staticmethod
    def _auto_kpm(X: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        # å…¨0 token ä½œä¸º padding
        return (X.abs().sum(dim=-1) <= eps)  # [NG, T], True=mask

    @staticmethod
    def _expand_kpm(kpm: Optional[torch.Tensor], B: int, G: int, T: int, device, dtype):
        if kpm is None:
            return None
        M = kpm.to(device=device, dtype=torch.bool)
        if M.dim() == 2:         # [B,T] -> [B,G,T] -> [NG,T]
            M = M.view(B, 1, T).expand(B, G, T).reshape(B*G, T)
        elif M.dim() == 3:       # [B,G,T] -> [NG,T]
            assert M.size(0) == B and M.size(1) == G and M.size(2) == T
            M = M.reshape(B*G, T)
        else:
            raise ValueError("key_padding_mask å¿…é¡»æ˜¯ [B,T] æˆ– [B,G,T]")
        return M

    @staticmethod
    def _safesqueeze_last(L: torch.Tensor):
        return L[..., 0] if (L.dim() >= 2 and L.size(-1) == 1) else L

    @staticmethod
    def _reduce_time(L: torch.Tensor, dim: int, how: str):
        if how == 'mean':
            return L.mean(dim=dim)
        elif how == 'last':
            return L.select(dim, L.size(dim)-1)
        else:
            raise ValueError("reduce must be mean/last")

    @staticmethod
    def _grow_pos_embed(param: nn.Parameter, need_T: int) -> nn.Parameter:
        cur_T = param.size(1)
        if need_T <= cur_T:
            return param
        # æ‰©å®¹ï¼šæ–°ä½ç½®åˆå§‹åŒ–ä¸ºæ­£æ€å™ªå£°
        new_pe = torch.zeros(param.size(0), need_T, device=param.device, dtype=param.dtype)
        nn.init.normal_(new_pe, std=0.02)
        new_pe[:, :cur_T] = param.data
        param.data = new_pe
        return param

    def _prep_labels(self, labels: torch.Tensor, B: int, G: int, T: int, device, dtype):
        """
        ç»Ÿä¸€åˆ°åºåˆ—çº§æ ‡é‡: [NG,1]
        æ”¯æŒ [B]/[B,1]/[B,G]/[B,G,1]/[B,T]/[B,G,T]/[B,G,T,1]
        """
        L = labels.to(device=device, dtype=dtype)
        L = self._safesqueeze_last(L)

        if L.dim() == 1:                         # [B]
            assert L.size(0) == B
            L = L.view(B, 1).expand(B, G)        # [B,G]
        elif L.dim() == 2:                       # [B,1]/[B,G]/[B,T]
            assert L.size(0) == B
            if L.size(1) == 1:
                L = L.expand(B, G)               # [B,G]
            elif L.size(1) == G:
                pass                             
            else:                                 # [B,T]
                L = self._reduce_time(L, dim=1, how=self.reduce).view(B, 1).expand(B, G)
        elif L.dim() == 3:                       # [B,G,T]
            assert L.size(0) == B and L.size(1) == G
            L = self._reduce_time(L, dim=2, how=self.reduce)   # [B,G]
        else:
            raise ValueError(f"unsupported label shape {tuple(L.shape)}")

        return L.reshape(B*G, 1)  # [NG,1]

    # ---------- forward ----------
    def forward(self,
                x: torch.Tensor,
                labels: torch.Tensor,
                pool_dim: int = 1,
                key_padding_mask: Optional[torch.Tensor] = None,
                eps: float = 1e-6):
        """
        è¿”å›:
        y_seq æˆ– y_pool, ä»¥åŠå¯é€‰ attn_weightsï¼ˆå¹³å‡åˆ° headï¼‰
        çº¦æŸ:
        - 3D è¾“å…¥ [B,T,D] ä»…å…è®¸ pool_dim=1 å¹¶åœ¨ dim=1 (T) ä¸Šæ± åŒ–
        - 4D è¾“å…¥ [B,G,T,D] ä»…å…è®¸ pool_dim=2 å¹¶åœ¨ dim=2 (T) ä¸Šæ± åŒ–
        """
        # ---- å½¢çŠ¶å±•å¼€ ----
        X, B, G, T, D, squeeze_3d = self._merge_BG(x)   # X:[NG,T,D]
        NG = B * G
        device, dtype = X.device, X.dtype

        # ---- pool_dim æ ¡éªŒ ----
        if squeeze_3d:
            assert pool_dim == 1, f"3D è¾“å…¥ [B,T,D] ä»…æ”¯æŒ pool_dim=1ï¼Œæ”¶åˆ° {pool_dim}"
        else:
            assert pool_dim == 2, f"4D è¾“å…¥ [B,G,T,D] ä»…æ”¯æŒ pool_dim=2ï¼Œæ”¶åˆ° {pool_dim}"

        # ---- KPMï¼šå¤–éƒ¨ kpm ä¸â€œå…¨é›¶è‡ªåŠ¨ maskâ€åˆå¹¶ ----
        kpm_ext = self._expand_kpm(key_padding_mask, B, G, T, device, dtype)
        kpm_auto = self._auto_kpm(X, eps=eps)
        kpm = kpm_auto if kpm_ext is None else (kpm_auto | kpm_ext)      # [NG,T], True=mask

        # ---- Q K V ----
        Q = self.Wq(X).reshape(NG, T, self.nhead, self.head_dim).transpose(1, 2)  # [NG,H,T,dh]
        K = self.Wk(X).reshape(NG, T, self.nhead, self.head_dim).transpose(1, 2)
        V = self.Wv(X).reshape(NG, T, self.nhead, self.head_dim).transpose(1, 2)

        # ---- (1) tau(label): logits / tau ----
        L_seq = self._prep_labels(labels, B, G, T, device, dtype)    # [NG,1]
        tau_raw = self.tau_mlp(L_seq)                                # [NG,1]
        tau = F.softplus(tau_raw) + self.tau_eps                     # >0
        Q = Q / tau.view(NG, 1, 1, 1).sqrt()

        # ---- (2) b_key(label): åˆ—åç½® ----
        b_scalar = self.bias_mlp(L_seq).view(NG, 1)                  # [NG,1]
        self._grow_pos_embed(self.pos_embed, T)                      # ç¡®ä¿ pos_embed è¶³å¤Ÿé•¿
        b_key = b_scalar * self.pos_embed[:, :T]                     # [1,T] -> [NG,T]
        b_key = b_key.expand(NG, T).contiguous()                     # [NG,T]

        # ---- SDPA ----
        q = Q.reshape(NG*self.nhead, T, self.head_dim)
        k = K.reshape(NG*self.nhead, T, self.head_dim)
        v = V.reshape(NG*self.nhead, T, self.head_dim)

        # æ„é€ åŠ æ€§ maskï¼šåˆ—åç½® + key paddingï¼ˆè¢«å±è”½çš„ key â†’ -infï¼‰
        logits_bias = b_key.unsqueeze(1).expand(NG, T, T)            # [NG,T,T]
        neg_inf = torch.finfo(dtype).min
        key_mask = kpm.unsqueeze(1).expand(NG, T, T)                 # True çš„åˆ—ç½® -inf
        logits_bias = logits_bias.masked_fill(key_mask, neg_inf)     # [NG,T,T]

        # ğŸ”§ ä¿®æ­£ï¼šæ²¿ head ç»´å¤åˆ¶ï¼Œå† reshape
        attn_mask = logits_bias.unsqueeze(1).expand(NG, self.nhead, T, T) \
                                    .reshape(NG * self.nhead, T, T)  # [NG*H, T, T]

        attn = F.scaled_dot_product_attention(
            q, k, v,
            attn_mask=attn_mask,
            dropout_p=0.0,
            is_causal=False
            )                                                         # [NG*H,T,dh]

        y = attn.reshape(NG, self.nhead, T, self.head_dim).transpose(1, 2).contiguous()
        y = y.reshape(NG, T, self.d_model)
        y = self.Wo(self.do(y))                                      # [NG,T,D]

        # ---- æ³¨æ„åŠ›æƒé‡ï¼ˆå¯é€‰ï¼Œå¹³å‡åˆ° headï¼‰----
        attn_weights = None
        if self.return_attn:
            qh = q / (self.head_dim ** 0.5)
            logits = torch.matmul(qh, k.transpose(-2, -1))           # [NG*H,T,T]
            logits = logits + logits_bias.reshape(NG*self.nhead, T, T)
            weights = torch.softmax(logits, dim=-1)                  # [NG*H,T,T]
            weights = weights.reshape(NG, self.nhead, T, T).mean(dim=1)  # [NG,T,T]
            attn_weights = weights if squeeze_3d else weights.reshape(B, G, T, T)

        # ---- è¿˜åŸå½¢çŠ¶ / æ± åŒ– ----
        if self.return_seq:
            if squeeze_3d:
                return (y, attn_weights) if self.return_attn else y   # [B,T,D]
            else:
                y = y.reshape(B, G, T, self.d_model)                  # [B,G,T,D]
                return (y, attn_weights) if self.return_attn else y
        else:
            # mean poolingï¼šå¯¹ 3D/4D éƒ½æ˜¯åœ¨æ—¶é—´ç»´ T ä¸Šæ± åŒ–
            # è¿™é‡Œæ˜¾å¼ç”¨ pool_dim åˆ¤æ–­ï¼Œä¿æŒå’Œä½ çš„æ¥å£è¯­ä¹‰ä¸€è‡´
            # 3D: pool_dim==1 â‡’ æ²¿ dim=1 æ± åŒ–ï¼›4D: pool_dim==2 â‡’ æ¯ä¸ª group å†…æ²¿æ—¶é—´æ± åŒ–
            mask = (~kpm).unsqueeze(-1).float()                       # [NG,T,1]
            safe = y * mask
            num = safe.sum(dim=1)                                     # [NG,D] (æ²¿ T)
            den = mask.sum(dim=1).clamp(min=1.0)
            pooled = num / den                                        # [NG,D]
            if squeeze_3d:
                return (pooled, attn_weights) if self.return_attn else pooled  # [B,D]
            else:
                pooled = pooled.view(B, G, self.d_model)              # [B,G,D]
                return (pooled, attn_weights) if self.return_attn else pooled

